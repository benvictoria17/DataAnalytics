{"cells":[{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"!pip install -q efficientnet_pytorch           ","execution_count":3,"outputs":[{"output_type":"stream","text":"\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\r\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade pip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# System\nimport cv2\nimport os, os.path\nfrom PIL import Image              # from RBG to YCbCr\nimport gc\nimport time\nimport datetime\n\n# Basics\nimport pandas as pd\nimport numpy as np\nimport random\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg    # to check images\n# %matplotlib inline\nfrom tqdm.notebook import tqdm      # beautiful progression bar\n\n# SKlearn\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import preprocessing\n\n# PyTorch\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import FloatTensor, LongTensor\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# Data Augmentation for Image Preprocessing\nfrom albumentations import (ToFloat, Normalize, VerticalFlip, HorizontalFlip, Compose, Resize,\n                            RandomBrightnessContrast, HueSaturationValue, Blur, GaussNoise,\n                            Rotate, RandomResizedCrop, Cutout, ShiftScaleRotate)\nfrom albumentations.pytorch import ToTensorV2, ToTensor\n\nfrom efficientnet_pytorch import EfficientNet\nfrom torchvision.models import resnet34, resnet50\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_seed(seed = 1234):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Device available now:', device)","execution_count":5,"outputs":[{"output_type":"stream","text":"Device available now: cpu\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ----- STATICS -----\noutput_size = 1\n# -------------------","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# My Train: with imputed missing values + OHE\nmy_train = pd.read_csv('../input/siim-melanoma-prep-data/train_clean.csv')\n\n# Drop path columns and Diagnosis (it won't be available during TEST)\n# We'll rewrite them once the data is concatenated\nto_drop = ['path_dicom','path_jpeg', 'diagnosis']\nfor drop in to_drop:\n    if drop in my_train.columns :\n        my_train.drop([drop], axis=1, inplace=True)\n\n# Roman's Train: with added data for Malignant category\nroman_train = pd.read_csv('../input/../input/melanoma-external-malignant-256/train_concat.csv')\n\n\n# --- Before concatenatenating both together, let's preprocess roman_train ---\n# Replace NAN with 0 for patient_id\nroman_train['patient_id'] = roman_train['patient_id'].fillna(0)\n\n# OHE\nto_encode = ['sex', 'anatom_site_general_challenge']\nencoded_all = []\n\nroman_train[to_encode[0]] = roman_train[to_encode[0]].astype(str)\nroman_train[to_encode[1]] = roman_train[to_encode[1]].astype(str)\n\nlabel_encoder = LabelEncoder()\n\nfor column in to_encode:\n    encoded = label_encoder.fit_transform(roman_train[column])\n    encoded_all.append(encoded)\n    \nroman_train[to_encode[0]] = encoded_all[0]\nroman_train[to_encode[1]] = encoded_all[1]\n\n# Give all columns the same name\nroman_train.columns = my_train.columns\n\n\n# --- Concatenate info which is not available in my_train ---\ncommon_images = my_train['dcm_name'].unique()\nnew_data = roman_train[~roman_train['dcm_name'].isin(common_images)]\n\n# Merge all together\ntrain_df = pd.concat([my_train, new_data], axis=0)\n\n\n\n# --- Read in Test data (also cleaned, imputed, OHE) ---\ntest_df = pd.read_csv('../input/siim-melanoma-prep-data/test_clean.csv')\n\n# Drop columns\nfor drop in to_drop:\n    if drop in test_df.columns :\n        test_df.drop([drop], axis=1, inplace=True)\n\n# Create path column to image folder for both Train and Test\npath_train = '../input/melanoma-external-malignant-256/train/train/'\npath_test = '../input/melanoma-external-malignant-256/test/test/'\n\ntrain_df['path_jpg'] = path_train + train_df['dcm_name'] + '.jpg'\ntest_df['path_jpg'] = path_test + test_df['dcm_name'] + '.jpg'\n\n\n# --- Last final thing: NORMALIZE! ---\ntrain_df['age'] = train_df['age'].fillna(-1)\n\nnormalized_train = preprocessing.normalize(train_df[['sex', 'age', 'anatomy']])\nnormalized_test = preprocessing.normalize(test_df[['sex', 'age', 'anatomy']])\n\ntrain_df['sex'] = normalized_train[:, 0]\ntrain_df['age'] = normalized_train[:, 1]\ntrain_df['anatomy'] = normalized_train[:, 2]\n\ntest_df['sex'] = normalized_test[:, 0]\ntest_df['age'] = normalized_test[:, 1]\ntest_df['anatomy'] = normalized_test[:, 2]\n\n\nprint('Len Train: {:,}'.format(len(train_df)), '\\n' +\n      'Len Test: {:,}'.format(len(test_df)))\n\n# Yay!","execution_count":7,"outputs":[{"output_type":"stream","text":"Len Train: 37,648 \nLen Test: 10,982\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ----- STATICS -----\nvertical_flip = 0.5\nhorizontal_flip = 0.5\n\ncsv_columns = ['sex', 'age', 'anatomy']\nno_columns = 3\n# ------------------","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example of csv_data at index=0\nnp.array(train_df.iloc[0][csv_columns].values,dtype=np.float32)","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"array([0.02221674, 0.9997532 , 0.        ], dtype=float32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MelanomaDataset(Dataset):\n    \n    def __init__(self, dataframe, vertical_flip, horizontal_flip,\n                 is_train=True, is_valid=False, is_test=False):\n        self.dataframe, self.is_train, self.is_valid = dataframe, is_train, is_valid\n        self.vertical_flip, self.horizontal_flip = vertical_flip, horizontal_flip\n        \n        # Data Augmentation (custom for each dataset type)\n        if is_train or is_test:\n            self.transform = Compose([RandomResizedCrop(height=224, width=224, scale=(0.4, 1.0)),\n                                      ShiftScaleRotate(rotate_limit=90, scale_limit = [0.8, 1.2]),\n                                      HorizontalFlip(p = self.horizontal_flip),\n                                      VerticalFlip(p = self.vertical_flip),\n                                      HueSaturationValue(sat_shift_limit=[0.7, 1.3], \n                                                         hue_shift_limit=[-0.1, 0.1]),\n                                      RandomBrightnessContrast(brightness_limit=[0.7, 1.3],\n                                                               contrast_limit= [0.7, 1.3]),\n                                      Normalize(),\n                                      ToTensor()])\n        else:\n            self.transform = Compose([Normalize(),\n                                      ToTensor()])\n            \n    def __len__(self):\n        return len(self.dataframe)\n    \n    def __getitem__(self, index):\n        # Select path and read image\n        image_path = self.dataframe['path_jpg'][index]\n        image = cv2.imread(image_path)\n        # For this image also import .csv information (sex, age, anatomy)\n        csv_data = np.array(self.dataframe.iloc[index][['sex', 'age', 'anatomy']].values, \n                            dtype=np.float32)\n        \n        # Apply transforms\n        image = self.transform(image=image)\n        # Extract image from dictionary\n        image = image['image']\n        \n        # If train/valid: image + class | If test: only image\n        if self.is_train or self.is_valid:\n            return (image, csv_data), self.dataframe['target'][index]\n        else:\n            return (image, csv_data)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ResNet50Network(nn.Module):\n    def __init__(self, output_size, no_columns):\n        super().__init__()\n        self.no_columns, self.output_size = no_columns, output_size\n        \n        # Define Feature part (IMAGE)\n        self.features = resnet50(pretrained=True) # 1000 neurons out\n        # (CSV data)\n        self.csv = nn.Sequential(nn.Linear(self.no_columns, 500),\n                                 nn.BatchNorm1d(500),\n                                 nn.ReLU(),\n                                 nn.Dropout(p=0.2))\n        \n        # Define Classification part\n        self.classification = nn.Linear(1000 + 500, output_size)\n        \n        \n    def forward(self, image, csv_data, prints=False):\n        \n        if prints: print('Input Image shape:', image.shape, '\\n'+\n                         'Input csv_data shape:', csv_data.shape)\n        \n        # Image CNN\n        image = self.features(image)\n        if prints: print('Features Image shape:', image.shape)\n        \n        # CSV FNN\n        csv_data = self.csv(csv_data)\n        if prints: print('CSV Data:', csv_data.shape)\n            \n        # Concatenate layers from image with layers from csv_data\n        image_csv_data = torch.cat((image, csv_data), dim=1)\n        \n        # CLASSIF\n        out = self.classification(image_csv_data)\n        if prints: print('Out shape:', out.shape)\n        \n        return out","execution_count":11,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"model_example = ResNet50Network(output_size=output_size, no_columns=no_columns)","execution_count":12,"outputs":[{"output_type":"stream","text":"Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"491f67d3323f42869f6ff957ba032f3c"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data object and Loader\nexample_data = MelanomaDataset(train_df, vertical_flip=0.5, horizontal_flip=0.5, \n                               is_train=True, is_valid=False, is_test=False)\nexample_loader = torch.utils.data.DataLoader(example_data, batch_size = 3, shuffle=True)\n\n# Get a sample\nfor (image, csv_data), labels in example_loader:\n    image_example, csv_data_example = image, csv_data\n    labels_example = torch.tensor(labels, dtype=torch.float32)\n    break\nprint('Data shape:', image_example.shape, '| \\n' , csv_data_example)\nprint('Label:', labels_example, '\\n')\n\n# Outputs\nout = model_example(image_example, csv_data_example, prints=True)\n\n# Criterion example\ncriterion_example = nn.BCEWithLogitsLoss()\n# Unsqueeze(1) from shape=[3] to shape=[3, 1]\nloss = criterion_example(out, labels_example.unsqueeze(1))   \nprint('Loss:', loss.item())","execution_count":13,"outputs":[{"output_type":"stream","text":"Data shape: torch.Size([3, 3, 224, 224]) | \n tensor([[0.0000, 0.9994, 0.0333],\n        [0.0000, 0.9999, 0.0143],\n        [0.0000, 0.9685, 0.2490]])\nLabel: tensor([0., 0., 1.]) \n\nInput Image shape: torch.Size([3, 3, 224, 224]) \nInput csv_data shape: torch.Size([3, 3])\nFeatures Image shape: torch.Size([3, 1000])\nCSV Data: torch.Size([3, 500])\nOut shape: torch.Size([3, 1])\nLoss: 1.1359907388687134\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class EfficientNetwork(nn.Module):\n    def __init__(self, output_size, no_columns, b4=False, b2=False):\n        super().__init__()\n        self.b4, self.b2, self.no_columns = b4, b2, no_columns\n        \n        # Define Feature part (IMAGE)\n        if b4:\n            self.features = EfficientNet.from_pretrained('efficientnet-b4')\n        elif b2:\n            self.features = EfficientNet.from_pretrained('efficientnet-b2')\n        else:\n            self.features = EfficientNet.from_pretrained('efficientnet-b7')\n        \n        # (CSV)\n        self.csv = nn.Sequential(nn.Linear(self.no_columns, 250),\n                                 nn.BatchNorm1d(250),\n                                 nn.ReLU(),\n                                 nn.Dropout(p=0.2),\n                                 \n                                 nn.Linear(250, 250),\n                                 nn.BatchNorm1d(250),\n                                 nn.ReLU(),\n                                 nn.Dropout(p=0.2))\n        \n        # Define Classification part\n        if b4:\n            self.classification = nn.Sequential(nn.Linear(1792 + 250, output_size))\n        elif b2:\n            self.classification = nn.Sequential(nn.Linear(1408 + 250, output_size))\n        else:\n            self.classification = nn.Sequential(nn.Linear(2560 + 250, output_size))\n        \n        \n    def forward(self, image, csv_data, prints=False):    \n        \n        if prints: print('Input Image shape:', image.shape, '\\n'+\n                         'Input csv_data shape:', csv_data.shape)\n        \n        # IMAGE CNN\n        image = self.features.extract_features(image)\n        if prints: print('Features Image shape:', image.shape)\n            \n        if self.b4:\n            image = F.avg_pool2d(image, image.size()[2:]).reshape(-1, 1792)\n        elif self.b2:\n            image = F.avg_pool2d(image, image.size()[2:]).reshape(-1, 1408)\n        else:\n            image = F.avg_pool2d(image, image.size()[2:]).reshape(-1, 2560)\n        if prints: print('Image Reshaped shape:', image.shape)\n            \n        # CSV FNN\n        csv_data = self.csv(csv_data)\n        if prints: print('CSV Data:', csv_data.shape)\n            \n        # Concatenate\n        image_csv_data = torch.cat((image, csv_data), dim=1)\n        \n        # CLASSIF\n        out = self.classification(image_csv_data)\n        if prints: print('Out shape:', out.shape)\n        \n        return out","execution_count":14,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Create an example model - Effnet\nmodel_example = EfficientNetwork(output_size=output_size, no_columns=no_columns,\n                                 b4=False, b2=True)","execution_count":15,"outputs":[{"output_type":"stream","text":"Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b2-8bb594d6.pth\" to /root/.cache/torch/checkpoints/efficientnet-b2-8bb594d6.pth\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=36804509.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de0216dd0b6f43b4aa9a4a93a1cf4c0f"}},"metadata":{}},{"output_type":"stream","text":"\nLoaded pretrained weights for efficientnet-b2\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data object and Loader\nexample_data = MelanomaDataset(train_df, vertical_flip=0.5, horizontal_flip=0.5, \n                               is_train=True, is_valid=False, is_test=False)\nexample_loader = torch.utils.data.DataLoader(example_data, batch_size = 3, shuffle=True)\n\n# Get a sample\nfor (image, csv_data), labels in example_loader:\n    image_example, csv_data_example = image, csv_data\n    labels_example = torch.tensor(labels, dtype=torch.float32)\n    break\nprint('Data shape:', image_example.shape, '| \\n' , csv_data_example)\nprint('Label:', labels_example, '\\n')\n\n# Outputs\nout = model_example(image_example, csv_data_example, prints=True)\n\n# Criterion example\ncriterion_example = nn.BCEWithLogitsLoss()\n# Unsqueeze(1) from shape=[3] to shape=[3, 1]\nloss = criterion_example(out, labels_example.unsqueeze(1))   \nprint('Loss:', loss.item())","execution_count":16,"outputs":[{"output_type":"stream","text":"Data shape: torch.Size([3, 3, 224, 224]) | \n tensor([[0.0000, 0.9975, 0.0712],\n        [0.0000, 0.9981, 0.0614],\n        [0.0200, 0.9980, 0.0599]])\nLabel: tensor([0., 0., 1.]) \n\nInput Image shape: torch.Size([3, 3, 224, 224]) \nInput csv_data shape: torch.Size([3, 3])\nFeatures Image shape: torch.Size([3, 1408, 7, 7])\nImage Reshaped shape: torch.Size([3, 1408])\nCSV Data: torch.Size([3, 250])\nOut shape: torch.Size([3, 1])\nLoss: 0.730401337146759\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ----- STATICS -----\ntrain_len = len(train_df)\ntest_len = len(test_df)\n# -------------------\n\n\n# Out of Fold Predictions\noof = np.zeros(shape = (train_len, 1))\n\n# Predictions\npreds_submission = torch.zeros(size = (test_len, 1), dtype=torch.float32, device=device)\n\nprint('oof shape:', oof.shape, '\\n' +\n      'predictions shape:', preds_submission.shape)","execution_count":17,"outputs":[{"output_type":"stream","text":"oof shape: (37648, 1) \npredictions shape: torch.Size([10982, 1])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ----- STATICS -----\nk = 6              # number of folds in Group K Fold\n# -------------------","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Object\ngroup_fold = GroupKFold(n_splits = k)\n\n# Generate indices to split data into training and test set.\nfolds = group_fold.split(X = np.zeros(train_len), \n                         y = train_df['target'], \n                         groups = train_df['ID'].tolist())","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ----- STATICS -----\nepochs = 15\npatience = 3\nTTA = 3\nnum_workers = 8\nlearning_rate = 0.0005\nweight_decay = 0.0\nlr_patience = 1            # 1 model not improving until lr is decreasing\nlr_factor = 0.4            # by how much the lr is decreasing\n\nbatch_size1 = 32\nbatch_size2 = 16\n\nversion = 'v6'             # to keep tabs on versions\n# -------------------","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_folds(preds_submission, model, version = 'v1'):\n    # Creates a .txt file that will contain the logs\n    f = open(f\"logs_{version}.txt\", \"w+\")\n    \n    \n    for fold, (train_index, valid_index) in enumerate(folds):\n        # Append to .txt\n        with open(f\"logs_{version}.txt\", 'a+') as f:\n            print('-'*10, 'Fold:', fold+1, '-'*10, file=f)\n        print('-'*10, 'Fold:', fold+1, '-'*10)\n\n\n        # --- Create Instances ---\n        # Best ROC score in this fold\n        best_roc = None\n        # Reset patience before every fold\n        patience_f = patience\n        \n        # Initiate the model\n        model = model\n\n        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay=weight_decay)\n        scheduler = ReduceLROnPlateau(optimizer=optimizer, mode='max', \n                                      patience=lr_patience, verbose=True, factor=lr_factor)\n        criterion = nn.BCEWithLogitsLoss()\n\n\n        # --- Read in Data ---\n        train_data = train_df.iloc[train_index].reset_index(drop=True)\n        valid_data = train_df.iloc[valid_index].reset_index(drop=True)\n\n        # Create Data instances\n        train = MelanomaDataset(train_data, vertical_flip=vertical_flip, horizontal_flip=horizontal_flip, \n                                is_train=True, is_valid=False, is_test=False)\n        valid = MelanomaDataset(valid_data, vertical_flip=vertical_flip, horizontal_flip=horizontal_flip, \n                                is_train=False, is_valid=True, is_test=False)\n        # Read in test data | Remember! We're using data augmentation like we use for Train data.\n        test = MelanomaDataset(test_df, vertical_flip=vertical_flip, horizontal_flip=horizontal_flip,\n                               is_train=False, is_valid=False, is_test=True)\n\n        # Dataloaders\n        train_loader = DataLoader(train, batch_size=batch_size1, shuffle=True, num_workers=num_workers)\n        # shuffle=False! Otherwise function won't work!!!\n                # how do I know? ^^\n        valid_loader = DataLoader(valid, batch_size=batch_size2, shuffle=False, num_workers=num_workers)\n        test_loader = DataLoader(test, batch_size=batch_size2, shuffle=False, num_workers=num_workers)\n\n\n        # === EPOCHS ===\n        for epoch in range(epochs):\n            start_time = time.time()\n            correct = 0\n            train_losses = 0\n\n            # === TRAIN ===\n            # Sets the module in training mode.\n            model.train()\n\n            for (images, csv_data), labels in train_loader:\n                # Save them to device\n                images = torch.tensor(images, device=device, dtype=torch.float32)\n                csv_data = torch.tensor(csv_data, device=device, dtype=torch.float32)\n                labels = torch.tensor(labels, device=device, dtype=torch.float32)\n\n                # Clear gradients first; very important, usually done BEFORE prediction\n                optimizer.zero_grad()\n\n                # Log Probabilities & Backpropagation\n                out = model(images, csv_data)\n                loss = criterion(out, labels.unsqueeze(1))\n                loss.backward()\n                optimizer.step()\n\n                # --- Save information after this batch ---\n                # Save loss\n                train_losses += loss.item()\n                # From log probabilities to actual probabilities\n                train_preds = torch.round(torch.sigmoid(out)) # 0 and 1\n                # Number of correct predictions\n                correct += (train_preds.cpu() == labels.cpu().unsqueeze(1)).sum().item()\n\n            # Compute Train Accuracy\n            train_acc = correct / len(train_index)\n\n\n            # === EVAL ===\n            # Sets the model in evaluation mode\n            model.eval()\n\n            # Create matrix to store evaluation predictions (for accuracy)\n            valid_preds = torch.zeros(size = (len(valid_index), 1), device=device, dtype=torch.float32)\n\n\n            # Disables gradients (we need to be sure no optimization happens)\n            with torch.no_grad():\n                for k, ((images, csv_data), labels) in enumerate(valid_loader):\n                    images = torch.tensor(images, device=device, dtype=torch.float32)\n                    csv_data = torch.tensor(csv_data, device=device, dtype=torch.float32)\n                    labels = torch.tensor(labels, device=device, dtype=torch.float32)\n\n                    out = model(images, csv_data)\n                    pred = torch.sigmoid(out)\n                    valid_preds[k*images.shape[0] : k*images.shape[0] + images.shape[0]] = pred\n\n                # Compute accuracy\n                valid_acc = accuracy_score(valid_data['target'].values, \n                                           torch.round(valid_preds.cpu()))\n                # Compute ROC\n                valid_roc = roc_auc_score(valid_data['target'].values, \n                                          valid_preds.cpu())\n\n                # Compute time on Train + Eval\n                duration = str(datetime.timedelta(seconds=time.time() - start_time))[:7]\n\n\n                # PRINT INFO\n                # Append to .txt file\n                with open(f\"logs_{version}.txt\", 'a+') as f:\n                    print('{} | Epoch: {}/{} | Loss: {:.4} | Train Acc: {:.3} | Valid Acc: {:.3} | ROC: {:.3}'.\\\n                     format(duration, epoch+1, epochs, train_losses, train_acc, valid_acc, valid_roc), file=f)\n                # Print to console\n                print('{} | Epoch: {}/{} | Loss: {:.4} | Train Acc: {:.3} | Valid Acc: {:.3} | ROC: {:.3}'.\\\n                     format(duration, epoch+1, epochs, train_losses, train_acc, valid_acc, valid_roc))\n\n\n                # === SAVE MODEL ===\n\n                # Update scheduler (for learning_rate)\n                scheduler.step(valid_roc)\n\n                # Update best_roc\n                if not best_roc: # If best_roc = None\n                    best_roc = valid_roc\n                    torch.save(model.state_dict(),\n                               f\"Fold{fold+1}_Epoch{epoch+1}_ValidAcc_{valid_acc:.3f}_ROC_{valid_roc:.3f}.pth\")\n                    continue\n\n                if valid_roc > best_roc:\n                    best_roc = valid_roc\n                    # Reset patience (because we have improvement)\n                    patience_f = patience\n                    torch.save(model.state_dict(),\n                               f\"Fold{fold+1}_Epoch{epoch+1}_ValidAcc_{valid_acc:.3f}_ROC_{valid_roc:.3f}.pth\")\n                else:\n                    # Decrease patience (no improvement in ROC)\n                    patience_f = patience_f - 1\n                    if patience_f == 0:\n                        with open(f\"logs_{version}.txt\", 'a+') as f:\n                            print('Early stopping (no improvement since 3 models) | Best ROC: {}'.\\\n                                  format(best_roc), file=f)\n                        print('Early stopping (no improvement since 3 models) | Best ROC: {}'.\\\n                              format(best_roc))\n                        break\n\n\n        # === INFERENCE ===\n        # Choose model with best_roc in this fold\n        best_model_path = '../working/' + [file for file in os.listdir('../working') if str(round(best_roc, 3)) in file and 'Fold'+str(fold+1) in file][0]\n        # Using best model from Epoch Train\n        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n        model = EfficientNetwork(output_size = output_size, no_columns=no_columns,\n                         b4=False, b2=True).to(device)\n        model.load_state_dict(torch.load(best_model_path))\n        # Set the model in evaluation mode\n        model.eval()\n\n\n        with torch.no_grad():\n            # --- EVAL ---\n            # Predicting again on Validation data to get preds for OOF\n            valid_preds = torch.zeros(size = (len(valid_index), 1), device=device, dtype=torch.float32)\n\n            for k, ((images, csv_data), _) in enumerate(valid_loader):\n                images = torch.tensor(images, device=device, dtype=torch.float32)\n                csv_data = torch.tensor(csv_data, device=device, dtype=torch.float32)\n\n                out = model(images, csv_data)\n                pred = torch.sigmoid(out)\n                valid_preds[k*images.shape[0] : k*images.shape[0] + images.shape[0]] = pred\n\n            # Save info to OOF\n            oof[valid_index] = valid_preds.cpu().numpy()\n\n\n            # --- TEST ---\n            # Now (Finally) prediction for our TEST data\n            for i in range(TTA):\n                for k, (images, csv_data) in enumerate(test_loader):\n                    images = torch.tensor(images, device=device, dtype=torch.float32)\n                    csv_data = torch.tensor(csv_data, device=device, dtype=torch.float32)\n\n                    out = model(images, csv_data)\n                    # Covert to probablities\n                    out = torch.sigmoid(out)\n\n                    # ADDS! the prediction to the matrix we already created\n                    preds_submission[k*images.shape[0] : k*images.shape[0] + images.shape[0]] += out\n\n\n            # Divide Predictions by TTA (to average the results during TTA)\n            preds_submission /= TTA\n\n\n        # === CLEANING ===\n        # Clear memory\n        del train, valid, train_loader, valid_loader, images, labels\n        # Garbage collector\n        gc.collect()","execution_count":21,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### #Training in progress... 🛫 Please do not disturb"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# --- EffNet B2 ---\nmodel = EfficientNetwork(output_size = output_size, no_columns=no_columns,\n                         b4=False, b2=True).to(device)\n\n# # ===== Uncomment and Train =====\n# train_folds(preds_submission = preds_submission, model = model, version = version)\n\n# # Save OOF values\n# save_oof = pd.DataFrame(data = oof, columns=['oof'])\n# save_oof.to_csv(f'oof_{version}.csv', index=False)","execution_count":22,"outputs":[{"output_type":"stream","text":"Loaded pretrained weights for efficientnet-b2\n","name":"stdout"}]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Print the logs during training\nf = open('../input/siim-melanoma-prep-data/logs_v7.txt', \"r\")\ncontents = f.read()\nprint(contents)","execution_count":23,"outputs":[{"output_type":"stream","text":"---------- Fold: 1 ----------\n0:05:00 | Epoch: 1/15 | Loss: 246.9 | Train Acc: 0.91 | Valid Acc: 0.916 | ROC: 0.953\n0:04:54 | Epoch: 2/15 | Loss: 224.1 | Train Acc: 0.919 | Valid Acc: 0.951 | ROC: 0.97\n0:04:53 | Epoch: 3/15 | Loss: 217.7 | Train Acc: 0.921 | Valid Acc: 0.929 | ROC: 0.963\n0:05:00 | Epoch: 4/15 | Loss: 212.3 | Train Acc: 0.924 | Valid Acc: 0.949 | ROC: 0.962\n0:04:57 | Epoch: 5/15 | Loss: 200.1 | Train Acc: 0.93 | Valid Acc: 0.931 | ROC: 0.961\nEarly stopping (no improvement since 3 models) | Best ROC: 0.969582512315271\n---------- Fold: 2 ----------\n0:04:59 | Epoch: 1/15 | Loss: 234.0 | Train Acc: 0.917 | Valid Acc: 0.961 | ROC: 0.965\n0:04:54 | Epoch: 2/15 | Loss: 227.1 | Train Acc: 0.918 | Valid Acc: 0.966 | ROC: 0.957\n0:04:49 | Epoch: 3/15 | Loss: 217.2 | Train Acc: 0.922 | Valid Acc: 0.96 | ROC: 0.965\n0:04:49 | Epoch: 4/15 | Loss: 208.6 | Train Acc: 0.925 | Valid Acc: 0.975 | ROC: 0.971\n0:04:46 | Epoch: 5/15 | Loss: 202.7 | Train Acc: 0.928 | Valid Acc: 0.975 | ROC: 0.971\n0:04:45 | Epoch: 6/15 | Loss: 205.1 | Train Acc: 0.926 | Valid Acc: 0.972 | ROC: 0.973\n0:04:47 | Epoch: 7/15 | Loss: 200.2 | Train Acc: 0.927 | Valid Acc: 0.973 | ROC: 0.969\n0:04:49 | Epoch: 8/15 | Loss: 202.7 | Train Acc: 0.928 | Valid Acc: 0.976 | ROC: 0.969\n0:04:57 | Epoch: 9/15 | Loss: 196.2 | Train Acc: 0.929 | Valid Acc: 0.976 | ROC: 0.973\n0:04:55 | Epoch: 10/15 | Loss: 197.9 | Train Acc: 0.928 | Valid Acc: 0.976 | ROC: 0.972\n0:04:52 | Epoch: 11/15 | Loss: 192.3 | Train Acc: 0.93 | Valid Acc: 0.977 | ROC: 0.972\n0:04:51 | Epoch: 12/15 | Loss: 192.3 | Train Acc: 0.93 | Valid Acc: 0.976 | ROC: 0.972\nEarly stopping (no improvement since 3 models) | Best ROC: 0.9727200147279357\n---------- Fold: 3 ----------\n0:04:52 | Epoch: 1/15 | Loss: 211.7 | Train Acc: 0.926 | Valid Acc: 0.976 | ROC: 0.977\n0:04:55 | Epoch: 2/15 | Loss: 211.5 | Train Acc: 0.924 | Valid Acc: 0.977 | ROC: 0.974\n0:04:56 | Epoch: 3/15 | Loss: 208.8 | Train Acc: 0.926 | Valid Acc: 0.973 | ROC: 0.978\n0:04:51 | Epoch: 4/15 | Loss: 205.8 | Train Acc: 0.926 | Valid Acc: 0.976 | ROC: 0.972\n0:04:45 | Epoch: 5/15 | Loss: 211.6 | Train Acc: 0.923 | Valid Acc: 0.972 | ROC: 0.976\n0:04:44 | Epoch: 6/15 | Loss: 198.9 | Train Acc: 0.929 | Valid Acc: 0.975 | ROC: 0.976\nEarly stopping (no improvement since 3 models) | Best ROC: 0.9777181996522981\n---------- Fold: 4 ----------\n0:04:45 | Epoch: 1/15 | Loss: 206.0 | Train Acc: 0.925 | Valid Acc: 0.969 | ROC: 0.974\n0:04:45 | Epoch: 2/15 | Loss: 204.1 | Train Acc: 0.927 | Valid Acc: 0.964 | ROC: 0.969\n0:04:46 | Epoch: 3/15 | Loss: 202.1 | Train Acc: 0.927 | Valid Acc: 0.97 | ROC: 0.96\n0:04:54 | Epoch: 4/15 | Loss: 193.6 | Train Acc: 0.931 | Valid Acc: 0.972 | ROC: 0.974\n0:04:49 | Epoch: 5/15 | Loss: 193.0 | Train Acc: 0.93 | Valid Acc: 0.972 | ROC: 0.973\n0:04:50 | Epoch: 6/15 | Loss: 189.8 | Train Acc: 0.931 | Valid Acc: 0.972 | ROC: 0.974\n0:04:49 | Epoch: 7/15 | Loss: 185.0 | Train Acc: 0.933 | Valid Acc: 0.97 | ROC: 0.974\nEarly stopping (no improvement since 3 models) | Best ROC: 0.9742626009369002\n---------- Fold: 5 ----------\n0:04:48 | Epoch: 1/15 | Loss: 206.1 | Train Acc: 0.925 | Valid Acc: 0.971 | ROC: 0.972\n0:04:46 | Epoch: 2/15 | Loss: 206.7 | Train Acc: 0.925 | Valid Acc: 0.969 | ROC: 0.975\n0:04:47 | Epoch: 3/15 | Loss: 198.9 | Train Acc: 0.929 | Valid Acc: 0.972 | ROC: 0.97\n0:04:46 | Epoch: 4/15 | Loss: 202.3 | Train Acc: 0.927 | Valid Acc: 0.977 | ROC: 0.977\n0:04:47 | Epoch: 5/15 | Loss: 201.2 | Train Acc: 0.927 | Valid Acc: 0.971 | ROC: 0.973\n0:04:53 | Epoch: 6/15 | Loss: 197.1 | Train Acc: 0.93 | Valid Acc: 0.966 | ROC: 0.971\n0:05:06 | Epoch: 7/15 | Loss: 191.2 | Train Acc: 0.93 | Valid Acc: 0.975 | ROC: 0.979\n0:04:59 | Epoch: 8/15 | Loss: 191.2 | Train Acc: 0.931 | Valid Acc: 0.976 | ROC: 0.979\n0:05:03 | Epoch: 9/15 | Loss: 191.3 | Train Acc: 0.932 | Valid Acc: 0.974 | ROC: 0.979\n0:05:06 | Epoch: 10/15 | Loss: 188.6 | Train Acc: 0.932 | Valid Acc: 0.973 | ROC: 0.98\n0:05:12 | Epoch: 11/15 | Loss: 186.3 | Train Acc: 0.933 | Valid Acc: 0.972 | ROC: 0.979\n0:05:06 | Epoch: 12/15 | Loss: 187.8 | Train Acc: 0.932 | Valid Acc: 0.977 | ROC: 0.983\n0:05:03 | Epoch: 13/15 | Loss: 183.6 | Train Acc: 0.934 | Valid Acc: 0.974 | ROC: 0.978\n0:05:00 | Epoch: 14/15 | Loss: 186.1 | Train Acc: 0.932 | Valid Acc: 0.973 | ROC: 0.98\n0:04:59 | Epoch: 15/15 | Loss: 183.3 | Train Acc: 0.933 | Valid Acc: 0.975 | ROC: 0.981\nEarly stopping (no improvement since 3 models) | Best ROC: 0.9830294890684985\n---------- Fold: 6 ----------\n0:05:00 | Epoch: 1/15 | Loss: 202.1 | Train Acc: 0.929 | Valid Acc: 0.974 | ROC: 0.983\n0:05:00 | Epoch: 2/15 | Loss: 202.6 | Train Acc: 0.927 | Valid Acc: 0.981 | ROC: 0.986\n0:04:59 | Epoch: 3/15 | Loss: 203.0 | Train Acc: 0.926 | Valid Acc: 0.977 | ROC: 0.984\n0:04:59 | Epoch: 4/15 | Loss: 198.5 | Train Acc: 0.93 | Valid Acc: 0.982 | ROC: 0.987\n0:05:00 | Epoch: 5/15 | Loss: 197.4 | Train Acc: 0.929 | Valid Acc: 0.981 | ROC: 0.986\n0:05:00 | Epoch: 6/15 | Loss: 198.9 | Train Acc: 0.928 | Valid Acc: 0.972 | ROC: 0.983\n0:05:00 | Epoch: 7/15 | Loss: 191.6 | Train Acc: 0.931 | Valid Acc: 0.979 | ROC: 0.984\nEarly stopping (no improvement since 3 models) | Best ROC: 0.9868998283213488\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # --- EffNet B4 ---\n# model = EfficientNetwork(output_size = output_size, no_columns=no_columns,\n#                          b4=True, b2=False).to(device)\n\n# # Uncomment and Train\n# train_folds(preds_submission = preds_submission, model = model, version = version)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # --- ResNet50 ---\n# model = ResNet50Network(output_size=output_size, no_columns=no_columns).to(device)\n\n# # Uncomment and Train\n# train_folds(preds_submission = preds_submission, model = model, version = version)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import OOF (pretrained)\noof = pd.read_csv('../input/siim-melanoma-prep-data/oof_v7.csv')\n\n# ROC on full Training data\nprint('OOF ROC: {:.3f}'.format(roc_auc_score(train_df['target'], oof)))","execution_count":26,"outputs":[{"output_type":"stream","text":"OOF ROC: 0.976\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make OOF Binary\noof.loc[oof.oof >= 0.5, 'oof'] = 1\noof.loc[oof.oof < 0.5, 'oof'] = 0\n\n# Create Confusion Matrix\ncf_matrix = confusion_matrix(train_df['target'], oof)\n\n# Pretty CM:\ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\n# Format of the absolute numbers\ngroup_counts = ['{:,}'.format(value) for value in cf_matrix.flatten()]\n# Format for relative numbers\ngroup_percentages = ['{0:.1%}'.format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\n\n# --- The figure ---\nplt.figure(figsize=(16, 5))\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Oranges',xticklabels=['benign', 'malignant'], \n            yticklabels=['benign', 'malignant'], cbar=False)\n\nmatplotlib.rcParams.update({'font.size': 15})\nplt.tick_params(axis='both', labelsize=15)\nplt.title('Confusion Matrix: OOF Data', fontsize=20);","execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1152x360 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Divide predictions by the number of folds\npreds_submission /= k\npreds_submission = preds_submission.cpu().numpy().reshape(-1,)\n\n# Import submission file\nss = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv')\n\nss['target'] = preds_submission\nss.to_csv(f'submission_{version}.csv', index=False)","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def best_single_model(model, preds_submission, TTA=3):\n    '''Function that takes an input model (trained) and makes the prediction for submission.'''\n    \n    test = MelanomaDataset(test_df, vertical_flip=0.5, horizontal_flip=0.5,\n                           is_train=False, is_valid=False, is_test=True)\n    test_loader = DataLoader(test, batch_size=16, shuffle=False, num_workers=8)\n    \n    model.eval()\n\n    with torch.no_grad():\n        for i in range(TTA):\n            for k, (images, csv_data) in enumerate(test_loader):\n                images = torch.tensor(images, device=device, dtype=torch.float32)\n                csv_data = torch.tensor(csv_data, device=device, dtype=torch.float32)\n\n                out = model(images, csv_data)\n                # Covert to probablities\n                out = torch.sigmoid(out)\n\n                # ADDS! the prediction to the matrix we already created\n                preds_submission[k*images.shape[0] : k*images.shape[0] + images.shape[0]] += out\n\n\n        # Divide Predictions by TTA (to average the results during TTA)\n        preds_submission /= TTA\n        \n    return preds_submission","execution_count":29,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}